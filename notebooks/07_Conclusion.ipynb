{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Máster en Data Science - Machine Learning\n",
    "\n",
    "## Predicción de impago de crédito mediante la implementación de modelos de Machine Learning\n",
    "Autor: Ramón Morillo Barrera\n",
    "\n",
    "## Dataset: Application data\n",
    "\n",
    "# **Conclusión Final**\n",
    "\n",
    "Ya finalizadas todas las etapas de este proyecto de Machine Learning, en las que se ha redactado un caso de negocio explicando la importancia de resolver esta problemática, se han expuesto los objetivos, los pasos y las métricas a tener en cuenta. Voy a realizar una recapitulación de todas las fases que comprendieron este proyecto recalcando los procedimientos llevados a cabo y argumentando las conclusiones más importantes que se han hallado.\n",
    "\n",
    "En primer lugar, el objetivo principal de este proyecto se ha centrado en poner en práctica todo lo aprendido sobre esta ciencia la cuál me ha permitido investigar, resolver problemas de manera autónoma, tomar decisiones fundamentadas y ajustar un problema real a unas soluciones de negocio. En este proceso he disfrutado y comprendido muchos conceptos que son necesarios para llevar a cabo proyectos de Machine Learning. A su vez, he entendido la importancia de escribir código de manera ordenada y encapsulada, la cuál permite llevar una estructura clara y aumentar la eficiencia de este tipo de proyectos. \n",
    "\n",
    "En cuanto a los hallazgos encontrados en base a los datos, En el primer EDA nos encontramos con un claro desbalanceo de nuestra variable objetivo, pues la clase minoritaria que muestra el impago es un 8.07%, esto dificultaría el entrenamiento del modelo ya que disponemos de una clase con una proporción muy pequeña para poder predecir. Además de implementar el uso de escalas logarítmicas a la hora de graficar visualizaciones que supongan una comparación de 'X' variables con nuestra variable objetivo. A pesar de ello, con investigación propia y ayuda de los conceptos visto en clase, he podido completar las etapas del proyecto de manera exitosa, aunque no con resultados perfectos.\n",
    "\n",
    "Al finalizar el EDA, vimos varias tendencias de ciertas variables que influían en nuestra variable objetivo, generamos varias hipótesis que posteriormente serían contrastadas acerca de variables como los documentos `EXT_SOURCE`, la educación del cliente `NAME_EDUCATION_TYPE`, el trabajo del cliente `OCCUPATION_TYPE`, la cantidad de crédito solicitada `AMT_CREDIT`, o el valor de los bienes de nuestros clientes,`AMT_GOODS_PRICES`, entre otras variables. Todo esto me permitió comprender el significado de las variables y como afectaban en la probabilidad de impago, además de tener en cuenta las diferentes técnicas de corrección de outiers e imputación de valores nulos, dichos procedimientos mayoritariamente se tienen en cuenta según el problema de negocio que tenemos y según las variables que influyen en nuestra variable objetivo.\n",
    "\n",
    "Una vez terminado el EDA, pasamos al Feature Engineering (ingeniería y selección de variables). En mi caso fue la parte más larga y en las que mayores dudas me surgieron. Comencé documentándome acerca del encoding de variables, apliqué el encoding según las conclusiones a las que llegué y procedí a escalar mis variables con el objetivo de aplicar técnicas de reducción de dimensionalidad. Busqué información acerca de la reducción de dimensionalidad incidiendo en 3 principales métodos (PCA, T-SNE y K-MEANS), después de comprobar sus resultados la varianza explicada con pocos componentes no era muy alta, por lo que descarté el uso de las mismas. Apliqué métodos de regularización los cuales me mostrarían la importancia de las diferentes variables de mis datos. Con ello conseguí realizar diferentes pruebas alojadas en la carpeta *experiments* en las que decidí seleccionar las variables de mi modelo que mayor explicabilidad proporcionaron, en mi caso particular, 21 variables. Esto ayudaría a los modelos a entender de una mejor manera los datos que recibe y a procesarlos computacionalmente más rápido. Por último, creé un Pipeline compuesto de 2 funciones donde encapsulé los tratamientos realizados a los datos lo que me permite aplicar dicho procesamiento en una sola línea de código, con esto me refería a lo arriba descrito de aprender a escribir código de manera ordenada y encapsulada.\n",
    "\n",
    "Con los datos ya procesados y seleccionados se probaron varios modelos, apliqué técnicas de undersampling y oversampling para comparar resultados. Hice otro experimento con una muestra de nuestro dataset completo incluyendo y excluyendo variables que a priori, demostraban un comportamiento extraño. Esto me permitió probar y comprender el funcionamiento de muchos modelos, de los cuáles me centré en modelos de clasificación ya que, en mi caso particular fueron los que mejores resultados me proporcionaron. A la hora de la selección de los mismos incidí en la importancia de métricas como el Recall y el F2score ya que buscaba el mayor número de predicciones de impago intentando predecir la menor cantidad Falsos Positivos, aunque ello conllevara un sacrificio del Acuraccy. Esta decisión la tomé ya que si extrapolamos esta problemática a negocio, todos los falsos positivos deberían de ser investigados y ello conllevaría un gasto extra para la compañía. En cuanto a la disminución del Acuraccy, para mi no es una métrica tan importante en este tipo de datos que presentan un gran desbalanceo de la variable objetivo, pues sus resultados pueden estar sesgados. En base a estos criterios fundamentados y justificados, aunque el modelo que mejor predecía en primera instancia era el modelo XGBoost, su tasa de falsos positivos era mayor, por lo que acabé decidiéndome por implementar el modelo LightGBM como modelo final.\n",
    "\n",
    "Acabando con este apartado, he utilizado SHAP para generar y argumentar la explicabilidad de mi modelo. Se ha llevado a cabo un procedimiento de explicabilidad global, argumentando la importancia de las variables seleccionadas y como influían en la probabilidad de impago según el valor que tomasen. Gráficos de dependencias parciales en los que se han analizados variables independientes y su importancia en el modelo según sus valores. Por lo que hemos conseguido explicar el funcionamiento del modelo de manera adecuada, pero para mí, lo más interesante ha sido la aplicación de la explicabilidad de manera local. En la que he aprendido y he podido interpretar casos particulares de individuos según sus características, esto es interesante ya que ajustando este tipo de explicabilidad a individuos particulares puedes fundamentar que características de los mismos y en que medidas influyen en la probabilidad de impago a la hora de concederle un crédito bancario.\n",
    "\n",
    "Como conclusión final, he de decir que los resultados son notoriamente mejorables, pero este proyecto ha conseguido darme una visión más amplia de la gran capacidad de esta ciencia en la que se pueden resolver problemas complejos siguiendo diferentes metodologías y que, en la mayoría de casos los buenos resultados se consiguen contrastando hipótesis generadas mediante procedimientos de prueba y error. Es por ello que el hecho de aplicar un modelo y ajustar su parámetros comprende la mínima parte del trabajo de un Científico de Datos, en el que se recalca la verdadera importancia de comprender el funcionamiento del negocio, los requerimientos de la empresa, así como todo el ciclo de análisis y entendimiento de los datos además del procesamiento, selección y explicabilidad de variables. Esto es lo que verdaderamente toma importancia en un contexto empresarial real con problemáticas y requerimientos de negocio a los que hay que ajustarse.\n",
    "\n",
    "El siguiente y último paso de este proyecto se trata de poner en producción el modelo, proceso que se llevará a cabo con Flask en el que se tendrá que realizar una monitorización del mismo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
